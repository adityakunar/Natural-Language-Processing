{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the data\n",
    "roughDat= pd.read_csv(\"PlayDat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes unnecessary columns.\n",
    "roughDat = roughDat.drop(columns=['Unnamed: 0', 'articleHeadline', 'claimId',\n",
    "       'articleId', 'articleHeadlineStance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = r'D:\\\\Downloads\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re \n",
    "mylist=roughDat[\"claimHeadline\"].unique().tolist()\n",
    "#myshortlist = [re.sub(r'[^\\w\\s\\']','',s) for s in mylist] for removing punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp D:\\\\Downloads\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-3bf3696d73c04710.props -preload depparse\n"
     ]
    }
   ],
   "source": [
    "from stanfordnlp.server import CoreNLPClient\n",
    "rootIndex =[]\n",
    "rootDistances = []\n",
    "with CoreNLPClient(annotators=['depparse'], timeout=60000, memory='16G') as client:\n",
    "    # submit the request to the server\n",
    "    for sentences in mylist:\n",
    "\n",
    "        ann = client.annotate(sentences)\n",
    "\n",
    "        # get the first sentence\n",
    "        sentence = ann.sentence[0]\n",
    "        # get the dependency parse of the first sentence\n",
    "        dependency_parse = sentence.basicDependencies\n",
    "        root=int(dependency_parse.root[0])\n",
    "        rootIndex.append(root)\n",
    "        neg_Pos=None\n",
    "        for i in dependency_parse.edge:\n",
    "            if i.dep==\"neg\":\n",
    "                neg_Pos=int(i.target)\n",
    "        if neg_Pos!=None:\n",
    "            rootDist=abs(root - neg_Pos)\n",
    "        else:\n",
    "            rootDist=0\n",
    "        rootDistances.append(rootDist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootDistances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 2, 5, 5, 3, 4, 3, 2, 5, 4, 3, 6, 2, 3, 5, 3, 3, 6, 7, 7, 2, 3, 3, 3]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "refuting =['fake',\n",
    "           'fraud',\n",
    "           'hoax',\n",
    "           'false',\n",
    "           'deny', \n",
    "           'denies',\n",
    "           'refute',\n",
    "           'not',\n",
    "           'despite',\n",
    "           'nope',\n",
    "           'doubt', \n",
    "           'doubts',\n",
    "           'bogus',\n",
    "           'debunk',\n",
    "           'pranks',\n",
    "           'prank',\n",
    "           'retract']\n",
    "hedging = [\n",
    "        'alleged', 'allegedly',\n",
    "        'apparently',\n",
    "        'appear', 'appears',\n",
    "        'claim', 'claims',\n",
    "        'could',\n",
    "        'evidently',\n",
    "        'largely',\n",
    "        'likely',\n",
    "        'mainly',\n",
    "        'may', 'maybe', 'might',\n",
    "        'mostly',\n",
    "        'perhaps',\n",
    "        'presumably',\n",
    "        'probably',\n",
    "        'purported', 'purportedly',\n",
    "        'reported', 'reportedly',\n",
    "        'rumor', 'rumour', 'rumors', 'rumours', 'rumored', 'rumoured',\n",
    "        'says',\n",
    "        'seem',\n",
    "        'somewhat',\n",
    "        'supposedly',\n",
    "        'unconfirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytokenizedlist =[ word_tokenize(i) for i in mylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "hedged_RDist =[]\n",
    "k=0\n",
    "for i in mytokenizedlist:\n",
    "    Distances=[]\n",
    "    for j in hedging:\n",
    "        if j in i:\n",
    "            Dist=(abs(rootIndex[k]-i.index(j)))\n",
    "            Distances.append(Dist)\n",
    "        else:\n",
    "            Distances.append(0)\n",
    "    k+=1\n",
    "    Distances = [i for i in Distances if i != 0]\n",
    "    if len(Distances)==0:\n",
    "        Distances.append(0)\n",
    "    hedged_RDist.append(mean(Distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hedged_RDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hedged_RDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "refute_RDist =[]\n",
    "k=0\n",
    "for i in mytokenizedlist:\n",
    "    Distances=[]\n",
    "    for j in refuting:\n",
    "        if j in i:\n",
    "            Dist=(abs(rootIndex[k]-i.index(j)))\n",
    "            Distances.append(Dist)\n",
    "        else:\n",
    "            Distances.append(0)\n",
    "    k+=1\n",
    "    Distances = [i for i in Distances if i != 0]\n",
    "    if len(Distances)==0:\n",
    "        Distances.append(0)\n",
    "    refute_RDist.append(mean(Distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refute_RDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
