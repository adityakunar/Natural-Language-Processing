{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data\n",
    "df = pd.read_csv(\"../data/Emergent_NAACL2016/emergent/url-versions-2015-06-14-clean-train-fold-9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into claims and headlines\n",
    "claims, headlines = np.split(df[[\"claimHeadline\", \"articleHeadline\"]].values, 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case and tokenize claims and headlines\n",
    "claims_tok = [tokenizer.tokenize(claim.lower()) for claim in claims.flatten()]\n",
    "headlines_tok = [tokenizer.tokenize(headline.lower()) for headline in headlines.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem claims\n",
    "claims_tok_stemmed = []\n",
    "for claim in claims_tok:\n",
    "    claim_tok_stemmed = []\n",
    "    for tok in claim:\n",
    "        claim_tok_stemmed.append(stemmer.stem(tok))\n",
    "    claims_tok_stemmed.append(claim_tok_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem headlines\n",
    "headlines_tok_stemmed = []\n",
    "for headline in headlines_tok:\n",
    "    headline_tok_stemmed = []\n",
    "    for tok in headline:\n",
    "        headline_tok_stemmed.append(stemmer.stem(tok))\n",
    "    headlines_tok_stemmed.append(headline_tok_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pairs of claim and headline tokens\n",
    "pairs_tok = []\n",
    "for claim_tok, headline_tok in zip(claims_tok, headlines_tok):\n",
    "    pairs_tok.append(list(itertools.product(claim_tok, headline_tok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pairs of claim and headline stemmed tokens\n",
    "pairs_tok_stemmed = []\n",
    "for claim_tok_stemmed, headline_tok_stemmed in zip(claims_tok_stemmed, headlines_tok_stemmed):\n",
    "    pairs_tok_stemmed.append(list(itertools.product(claim_tok_stemmed, headline_tok_stemmed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ppdb data\n",
    "with open(\"../data/ppdb-2.0-xl-lexical.pkl\", \"rb\") as f:\n",
    "    ppdb_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in a pair of\n",
    "# headline and claim tokens(stemmed and non-stemmed)\n",
    "# and returns their ppdb score\n",
    "def ppdb_func(stem_pair, token_pair, ppdb_dict, max_score=10, min_score=-10):\n",
    "    paraphrase = ppdb_dict.get(token_pair[0], False)\n",
    "    if stem_pair[0] == stem_pair[1]:\n",
    "        return max_score\n",
    "    elif paraphrase and token_pair[1] in paraphrase:\n",
    "        ppdb_score = paraphrase[token_pair[1]][0]\n",
    "        return ppdb_score\n",
    "    else:\n",
    "        return min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(pairs_tok)\n",
    "alignment_feature = []\n",
    "\n",
    "# iterate over all samples\n",
    "for i in range(num_samples):\n",
    "    # get tokens for current sample\n",
    "    sample_tok = pairs_tok[i]\n",
    "    sample_stem = pairs_tok_stemmed[i]\n",
    "    num_pairs = len(sample_tok)\n",
    "\n",
    "    # matrix of ppdb scores of each claim-headline token pair\n",
    "    # each row represents a claim\n",
    "    # each column represents a headline\n",
    "    # score_matrix[n][m]: ppdb score for nth claim and mth headline for the ith sample\n",
    "    score_matrix = []\n",
    "\n",
    "    # iterate over all token pairs in sample\n",
    "    for j in range(num_pairs):\n",
    "        # get current pair\n",
    "        stem_pair = sample_stem[j]\n",
    "        token_pair = sample_tok[j]\n",
    "\n",
    "        # when one claim token is done,\n",
    "        # move to the next row of the score matrix\n",
    "        if j%len(headlines_tok[i]) == 0:\n",
    "            score_matrix.append([])\n",
    "        # get ppdb score between the pair of claim and headline\n",
    "        score_matrix[-1].append(ppdb_func(stem_pair, token_pair, ppdb_dict))\n",
    "    # after scores between all pairs,\n",
    "    # convert score matrix to a numpy array\n",
    "    score_matrix = np.array(score_matrix)\n",
    "    # compute the optimal assignment of pairs,\n",
    "    # using the hungarian algorithm\n",
    "    # here, use the negative of the score matrix because we want to maximize scores\n",
    "    # (hungarian algorithm, by default tries to minimize the cost matrix)\n",
    "    row_ind, col_ind = linear_sum_assignment(-score_matrix)\n",
    "    \n",
    "    # norm: min(length of claim, length of headline)\n",
    "    norm = min(len(claims_tok[i]), len(headlines_tok[i]))\n",
    "    # use the indices returned from the hungarian algorithm,\n",
    "    # to get optimal assignment and sum to get score of max 1-1 alignment\n",
    "    alignment_feature.append(score_matrix[row_ind, col_ind].sum()/norm)\n",
    "    \n",
    "    # TODO: neg feature from the stanfordnlp dependency parse\n",
    "    # input(np.array(claims_tok[i])[row_ind])\n",
    "    # input(np.array(headlines_tok[i])[col_ind])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
