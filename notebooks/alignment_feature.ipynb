{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordnlp.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data\n",
    "df = pd.read_csv(\"../data/Emergent_NAACL2016/emergent/url-versions-2015-06-14-clean-train-fold-9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into claims and headlines\n",
    "claims, headlines = np.split(df[[\"claimHeadline\", \"articleHeadline\"]].values, 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case and remove full stops\n",
    "claims = [re.sub(r\"\\.+$\", \"\", claim.lower()) for claim in claims.flatten().tolist()]\n",
    "headlines = [re.sub(r\"\\.+$\", \"\", headline.lower()) for headline in headlines.flatten().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize claims and headlines\n",
    "claims_tok = [word_tokenize(claim) for claim in claims]\n",
    "headlines_tok = [word_tokenize(headline) for headline in headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem claims\n",
    "claims_tok_stemmed = []\n",
    "for claim in claims_tok:\n",
    "    claim_tok_stemmed = []\n",
    "    for tok in claim:\n",
    "        claim_tok_stemmed.append(stemmer.stem(tok))\n",
    "    claims_tok_stemmed.append(claim_tok_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem headlines\n",
    "headlines_tok_stemmed = []\n",
    "for headline in headlines_tok:\n",
    "    headline_tok_stemmed = []\n",
    "    for tok in headline:\n",
    "        headline_tok_stemmed.append(stemmer.stem(tok))\n",
    "    headlines_tok_stemmed.append(headline_tok_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pairs of claim and headline tokens\n",
    "pairs_tok = []\n",
    "for claim_tok, headline_tok in zip(claims_tok, headlines_tok):\n",
    "    pairs_tok.append(list(itertools.product(claim_tok, headline_tok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pairs of claim and headline stemmed tokens\n",
    "pairs_tok_stemmed = []\n",
    "for claim_tok_stemmed, headline_tok_stemmed in zip(claims_tok_stemmed, headlines_tok_stemmed):\n",
    "    pairs_tok_stemmed.append(list(itertools.product(claim_tok_stemmed, headline_tok_stemmed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ppdb data\n",
    "with open(\"../data/ppdb-2.0-xl-lexical.pkl\", \"rb\") as f:\n",
    "    ppdb_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in a pair of\n",
    "# headline and claim tokens(stemmed and non-stemmed)\n",
    "# and returns their ppdb score\n",
    "def ppdb_func(stem_pair, token_pair, ppdb_dict, max_score=10, min_score=-10):\n",
    "    paraphrase = ppdb_dict.get(token_pair[0], False)\n",
    "    if stem_pair[0] == stem_pair[1]:\n",
    "        return max_score\n",
    "    elif paraphrase and token_pair[1] in paraphrase:\n",
    "        ppdb_score = paraphrase[token_pair[1]][0]\n",
    "        return ppdb_score\n",
    "    else:\n",
    "        return min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/nikilsaldanaha/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-c4c2e1467e8349ae.props -preload tokenize,ssplit,pos,depparse\n"
     ]
    },
    {
     "ename": "AnnotationException",
     "evalue": "Request is too long to be handled by server: 113693 characters. Max length is 100000 characters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/University/Sem2/information retrieval/projects/stance-detection/env/lib/python3.7/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                               timeout=(self.timeout*2)/1000, **kwargs)\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University/Sem2/information retrieval/projects/stance-detection/env/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://localhost:9000/?properties=%7B%27outputFormat%27%3A+%27serialized%27%7D",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnnotationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a5704f598585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtext_claims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'. '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtext_headlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'. '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_claims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_headlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University/Sem2/information retrieval/projects/stance-detection/env/lib/python3.7/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University/Sem2/information retrieval/projects/stance-detection/env/lib/python3.7/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnnotationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_properties_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnnotationException\u001b[0m: Request is too long to be handled by server: 113693 characters. Max length is 100000 characters."
     ]
    }
   ],
   "source": [
    "num_samples = len(pairs_tok)\n",
    "alignment_feature = []\n",
    "\n",
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:\n",
    "    \n",
    "    text_claims = '. '.join(claims)\n",
    "    text_headlines = '. '.join(headlines)\n",
    "    client.annotate(text_claims)\n",
    "    client.annotate(text_headlines)\n",
    "    input(\"done!\")\n",
    "    # iterate over all samples\n",
    "    for i in range(num_samples):\n",
    "        # get tokens for current sample\n",
    "        sample_tok = pairs_tok[i]\n",
    "        sample_stem = pairs_tok_stemmed[i]\n",
    "        num_pairs = len(sample_tok)\n",
    "\n",
    "        # matrix of ppdb scores of each claim-headline token pair\n",
    "        # each row represents a claim\n",
    "        # each column represents a headline\n",
    "        # score_matrix[n][m]: ppdb score for nth claim and mth headline for the ith sample\n",
    "        score_matrix = []\n",
    "\n",
    "        # iterate over all token pairs in sample\n",
    "        for j in range(num_pairs):\n",
    "            # get current pair\n",
    "            stem_pair = sample_stem[j]\n",
    "            token_pair = sample_tok[j]\n",
    "\n",
    "            # when one claim token is done,\n",
    "            # move to the next row of the score matrix\n",
    "            if j%len(headlines_tok[i]) == 0:\n",
    "                score_matrix.append([])\n",
    "            # get ppdb score between the pair of claim and headline\n",
    "            score_matrix[-1].append(ppdb_func(stem_pair, token_pair, ppdb_dict))\n",
    "        # after scores between all pairs,\n",
    "        # convert score matrix to a numpy array\n",
    "        score_matrix = np.array(score_matrix)\n",
    "        # compute the optimal assignment of pairs,\n",
    "        # using the hungarian algorithm\n",
    "        # here, use the negative of the score matrix because we want to maximize scores\n",
    "        # (hungarian algorithm, by default tries to minimize the cost matrix)\n",
    "        row_ind, col_ind = linear_sum_assignment(-score_matrix)\n",
    "\n",
    "        # norm: min(length of claim, length of headline)\n",
    "        norm = min(len(claims_tok[i]), len(headlines_tok[i]))\n",
    "        # use the indices returned from the hungarian algorithm,\n",
    "        # to get optimal assignment and sum to get score of max 1-1 alignment\n",
    "        alignment_feature.append(score_matrix[row_ind, col_ind].sum()/norm)\n",
    "\n",
    "        # TODO: neg feature from the stanfordnlp dependency parse\n",
    "#         input(np.array(claims_tok[i])[row_ind])\n",
    "#         input(np.array(headlines_tok[i])[col_ind])\n",
    "#         input(claims[i])\n",
    "#         input(headlines[i])\n",
    "#         input(row_ind)\n",
    "#         input(col_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
