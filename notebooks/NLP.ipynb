{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the data\n",
    "roughDat= pd.read_csv(\"../data/emergent/url-versions-2015-06-14-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes unnecessary columns.\n",
    "roughDat = roughDat.drop(columns=['Unnamed: 0', 'articleHeadline',\n",
    "       'articleId', 'articleHeadlineStance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks in the data if there is a question mark in the end.\n",
    "QFeature=[]\n",
    "for i in roughDat['claimHeadline']:\n",
    "    if i[-1]==\"?\":\n",
    "        QFeature.append(1)\n",
    "    else:\n",
    "        QFeature.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appends the QFeature to the claimHeadline dataframe.\n",
    "roughDat[\"Qmark\"]= QFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimHeadline</th>\n",
       "      <th>claimId</th>\n",
       "      <th>Qmark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple will sell 19 million Apple Watches in 2015</td>\n",
       "      <td>8faeb4b0-c41b-11e4-88c9-eb158a06b9a5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two Australian men kept a McDonald's Quarter P...</td>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two Australian men kept a McDonald's Quarter P...</td>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two Australian men kept a McDonald's Quarter P...</td>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two Australian men kept a McDonald's Quarter P...</td>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>A woman woke up during brain surgery and talke...</td>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>A woman woke up during brain surgery and talke...</td>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>A woman woke up during brain surgery and talke...</td>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>A woman woke up during brain surgery and talke...</td>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>A woman woke up during brain surgery and talke...</td>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2595 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          claimHeadline  \\\n",
       "0      Apple will sell 19 million Apple Watches in 2015   \n",
       "1     Two Australian men kept a McDonald's Quarter P...   \n",
       "2     Two Australian men kept a McDonald's Quarter P...   \n",
       "3     Two Australian men kept a McDonald's Quarter P...   \n",
       "4     Two Australian men kept a McDonald's Quarter P...   \n",
       "...                                                 ...   \n",
       "2590  A woman woke up during brain surgery and talke...   \n",
       "2591  A woman woke up during brain surgery and talke...   \n",
       "2592  A woman woke up during brain surgery and talke...   \n",
       "2593  A woman woke up during brain surgery and talke...   \n",
       "2594  A woman woke up during brain surgery and talke...   \n",
       "\n",
       "                                   claimId  Qmark  \n",
       "0     8faeb4b0-c41b-11e4-88c9-eb158a06b9a5      0  \n",
       "1     d54aaf40-b6a8-11e4-8507-b58af63d1078      0  \n",
       "2     d54aaf40-b6a8-11e4-8507-b58af63d1078      0  \n",
       "3     d54aaf40-b6a8-11e4-8507-b58af63d1078      0  \n",
       "4     d54aaf40-b6a8-11e4-8507-b58af63d1078      0  \n",
       "...                                    ...    ...  \n",
       "2590  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff      1  \n",
       "2591  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff      1  \n",
       "2592  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff      1  \n",
       "2593  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff      1  \n",
       "2594  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff      1  \n",
       "\n",
       "[2595 rows x 3 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first feature has been extracted.\n",
    "roughDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughDat[[\"claimId\", \"Qmark\"]].to_csv(r\"../datasets/claim_Qmark.csv\",index=False, doublequote=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights into bag of words\n",
    "The BOW model only considers if a known word occurs in a document or not. It does not care about meaning, context, and order in which they appear.\n",
    "This gives the insight that similar documents will have word counts similar to each other. In other words, the more similar the words in two documents, the more similar the documents can be.\n",
    "Limitations of BOW\n",
    "Semantic meaning: the basic BOW approach does not consider the meaning of the word in the document. It completely ignores the context in which it’s used. The same word can be used in multiple places based on the context or nearby words.\n",
    "Vector size: For a large document, the vector size can be huge resulting in a lot of computation and time. You may need to ignore words based on relevance to your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "allsentences=list(roughDat['claimHeadline'])\n",
    "X = vectorizer.fit_transform(allsentences)\n",
    "\n",
    "#if we want to convert our occurences to frequencies\n",
    "#tf_transformer = TfidfTransformer(use_idf=False).fit(X)\n",
    "#X_f = tf_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3881</th>\n",
       "      <th>3882</th>\n",
       "      <th>3883</th>\n",
       "      <th>3884</th>\n",
       "      <th>3885</th>\n",
       "      <th>3886</th>\n",
       "      <th>3887</th>\n",
       "      <th>3888</th>\n",
       "      <th>3889</th>\n",
       "      <th>3890</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8faeb4b0-c41b-11e4-88c9-eb158a06b9a5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d54aaf40-b6a8-11e4-8507-b58af63d1078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2595 rows × 3891 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   claimId  1  2  3  4  5  6  7  8  9  ...  \\\n",
       "0     8faeb4b0-c41b-11e4-88c9-eb158a06b9a5  0  0  0  0  0  0  0  0  0  ...   \n",
       "1     d54aaf40-b6a8-11e4-8507-b58af63d1078  0  0  0  0  0  0  0  0  0  ...   \n",
       "2     d54aaf40-b6a8-11e4-8507-b58af63d1078  0  0  0  0  0  0  0  0  0  ...   \n",
       "3     d54aaf40-b6a8-11e4-8507-b58af63d1078  0  0  0  0  0  0  0  0  0  ...   \n",
       "4     d54aaf40-b6a8-11e4-8507-b58af63d1078  0  0  0  0  0  0  0  0  0  ...   \n",
       "...                                    ... .. .. .. .. .. .. .. .. ..  ...   \n",
       "2590  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff  0  0  0  0  0  0  0  0  0  ...   \n",
       "2591  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff  0  0  0  0  0  0  0  0  0  ...   \n",
       "2592  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff  0  0  0  0  0  0  0  0  0  ...   \n",
       "2593  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff  0  0  0  0  0  0  0  0  0  ...   \n",
       "2594  7ca09170-93bc-11e4-b6bf-fdc8b7e3bcff  0  0  0  0  0  0  0  0  0  ...   \n",
       "\n",
       "     3881 3882 3883 3884 3885 3886 3887 3888 3889 3890  \n",
       "0       0    0    0    0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "2590    0    0    0    0    0    0    0    0    0    0  \n",
       "2591    0    0    0    0    0    0    0    0    0    0  \n",
       "2592    0    0    0    0    0    0    0    0    0    0  \n",
       "2593    0    0    0    0    0    0    0    0    0    0  \n",
       "2594    0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[2595 rows x 3891 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BogF=X_f.toarray()\n",
    "BogF=X.toarray()\n",
    "BogF.shape\n",
    "data = np.append(roughDat[[\"claimId\"]],BogF, axis=1)\n",
    "df = pd.DataFrame(data=data)\n",
    "df = df.rename(columns={0: \"claimId\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"../datasets/claim_BoGF.csv\",index=False, doublequote=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = r'~/Downloads/stanford-corenlp-full-2018-10-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp ~/Downloads/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-fe9c5dcb215240d2.props -preload tokenize,ssplit,pos,depparse\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c2d0bbb97e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenize'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ssplit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'depparse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'16G'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# submit the request to the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# get the first sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/Inforet/lib/python3.8/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/Inforet/lib/python3.8/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/Inforet/lib/python3.8/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mensure_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECK_ALIVE_TIMEOUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mPermanentlyFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timed out waiting for service to come alive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stanfordnlp.server import CoreNLPClient \n",
    "text= 'The panther is killing the deer.'\n",
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:\n",
    "    # submit the request to the server\n",
    "    ann = client.annotate(text)\n",
    "\n",
    "    # get the first sentence\n",
    "    sentence = ann.sentence[0]\n",
    "\n",
    "    # get the dependency parse of the first sentence\n",
    "    print('---')\n",
    "    print('dependency parse of first sentence')\n",
    "    dependency_parse = sentence.basicDependencies\n",
    "    print(dependency_parse)\n",
    "\n",
    "    #get the tokens of the first sentence\n",
    "    #note that 1 token is 1 node in the parse tree, nodes start at 1\n",
    "    print('---')\n",
    "    print('Tokens of first sentence')\n",
    "    for token in sentence.token :\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root=int(dependency_parse.root[0])\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "tokenized_text = tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg found\n"
     ]
    }
   ],
   "source": [
    "#dependency_parse.edge\n",
    "word =\"wife\"\n",
    "indexineed=tokenized_text.index(word)+1\n",
    "for e in dependency_parse.edge:\n",
    "    if (e.source==indexineed)and(e.dep==\"neg\"):\n",
    "        print(\"neg found\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 1\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 2\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 3\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 4\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 5\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 6\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 7\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 8\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 9\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 10\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 11\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 12\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 13\n",
       "}\n",
       "edge {\n",
       "  source: 2\n",
       "  target: 1\n",
       "  dep: \"nsubj\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 2\n",
       "  target: 8\n",
       "  dep: \"ccomp\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 4\n",
       "  target: 3\n",
       "  dep: \"amod\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 8\n",
       "  target: 4\n",
       "  dep: \"nsubj\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 8\n",
       "  target: 5\n",
       "  dep: \"cop\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 8\n",
       "  target: 6\n",
       "  dep: \"neg\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 8\n",
       "  target: 7\n",
       "  dep: \"det\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 8\n",
       "  target: 13\n",
       "  dep: \"nmod\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 13\n",
       "  target: 9\n",
       "  dep: \"case\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 13\n",
       "  target: 10\n",
       "  dep: \"compound\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 13\n",
       "  target: 11\n",
       "  dep: \"compound\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 13\n",
       "  target: 12\n",
       "  dep: \"compound\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "root: 2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from this we know how to do rootDist.  \n",
    "neg_Pos=None\n",
    "for i in dependency_parse.edge:\n",
    "    if i.dep==\"neg\":\n",
    "        neg_Pos=int(i.target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rootDist is done bitches!\n",
    "if neg_Pos!=None:\n",
    "    rootDist=abs(root - neg_Pos)\n",
    "else:\n",
    "    rootDist=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp D:\\\\Downloads\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-c866f32af4534489.props -preload tokenize,ssplit,pos,depparse\n",
      "Starting server with command: java -Xmx16G -cp D:\\\\Downloads\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-055eb48bb2694a96.props -preload tokenize,ssplit,pos,depparse\n",
      "Starting server with command: java -Xmx16G -cp D:\\\\Downloads\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-9abf041ad7284eb5.props -preload tokenize,ssplit,pos,depparse\n",
      "Starting server with command: java -Xmx16G -cp D:\\\\Downloads\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-d89ab109005340c5.props -preload tokenize,ssplit,pos,depparse\n",
      "Starting server with command: java -Xmx16G -cp D:\\\\Downloads\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-21415b8197984222.props -preload tokenize,ssplit,pos,depparse\n"
     ]
    }
   ],
   "source": [
    "from stanfordnlp.server import CoreNLPClient\n",
    "rootIndex =[]\n",
    "rootDistances = []\n",
    "for sentences in myshort:\n",
    "    with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:\n",
    "        # submit the request to the server\n",
    "        ann = client.annotate(sentences)\n",
    "\n",
    "        # get the first sentence\n",
    "        sentence = ann.sentence[0]\n",
    "        # get the dependency parse of the first sentence\n",
    "        dependency_parse = sentence.basicDependencies\n",
    "        root=int(dependency_parse.root[0])\n",
    "        rootIndex.append(root)\n",
    "        neg_Pos=None\n",
    "        for i in dependency_parse.edge:\n",
    "            if i.dep==\"neg\":\n",
    "                neg_Pos=int(i.target)\n",
    "        if neg_Pos!=None:\n",
    "            rootDist=abs(root - neg_Pos)\n",
    "        else:\n",
    "            rootDist=0\n",
    "        rootDistances.append(rootDist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist=roughDat[\"claimHeadline\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshort=mylist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Developers have limited access to Apple Watch sensors and features, to make simpler apps that don't drain battery\",\n",
       " 'The Apple Watch only receives notifications when on the wrist',\n",
       " 'Apple have different Watch bands for sale at launch',\n",
       " 'The Washington Post is developing a new app that will be pre-installed on the new Kindle Fire',\n",
       " 'Seven underage Bosnian girls got pregnant on a school trip']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myshort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootDistances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "refuting =[  'fake',\n",
    "                        'fraud',\n",
    "                        'hoax',\n",
    "                        'false',\n",
    "                        'deny', \n",
    "                        'denies',\n",
    "                        'refute',\n",
    "                        'not',\n",
    "                        'despite',\n",
    "                        'nope',\n",
    "                        'doubt', \n",
    "                        'doubts',\n",
    "                        'bogus',\n",
    "                        'debunk',\n",
    "                        'pranks',\n",
    "                        'retract']\n",
    "hedging = [\n",
    "        'alleged', 'allegedly',\n",
    "        'apparently',\n",
    "        'appear', 'appears',\n",
    "        'claim', 'claims',\n",
    "        'could',\n",
    "        'evidently',\n",
    "        'largely',\n",
    "        'likely',\n",
    "        'mainly',\n",
    "        'may', 'maybe', 'might',\n",
    "        'mostly',\n",
    "        'perhaps',\n",
    "        'presumably',\n",
    "        'probably',\n",
    "        'purported', 'purportedly',\n",
    "        'reported', 'reportedly',\n",
    "        'rumor', 'rumour', 'rumors', 'rumours', 'rumored', 'rumoured',\n",
    "        'says',\n",
    "        'seem',\n",
    "        'somewhat',\n",
    "        'supposedly',\n",
    "        'unconfirmed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = myshort[0]+\" unconfirmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Developers have limited access to Apple Watch sensors and features, to make simpler apps that don't drain battery unconfirmed\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "tokenized_text = tokenizer.tokenize(my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in hedging:\n",
    "    if i in tokenized_text:\n",
    "        print(tokenized_text.index(i))\n",
    "        \n",
    "for i in refuting:\n",
    "    if i in tokenized_text:\n",
    "        print(tokenized_text.index(i))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytokenizedlist = []\n",
    "\n",
    "for i in mylist:\n",
    "    mytokenizedlist.append(tokenizer.tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(mytokenizedlist, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.4697524e-03, -2.4015261e-03,  2.2079211e-03, -2.5894598e-04,\n",
       "       -1.6402277e-04,  2.5014305e-03, -1.7184011e-03, -1.3010562e-03,\n",
       "       -2.5564588e-03, -2.6113268e-03, -4.5046853e-03,  4.5513688e-04,\n",
       "       -4.1931923e-03,  6.2852935e-04, -1.9148126e-03, -4.6986830e-04,\n",
       "        4.0941951e-03,  4.6374830e-03, -1.2535239e-03,  4.5928182e-03,\n",
       "        1.1121318e-03,  1.3658134e-03,  3.7711444e-03, -2.3843031e-03,\n",
       "       -8.7081804e-04, -5.7833537e-04,  1.9569665e-03, -2.3816463e-03,\n",
       "        2.1298253e-03, -4.8048827e-03,  2.1339087e-03, -1.6164563e-03,\n",
       "       -2.9615846e-03,  9.5751748e-04,  7.9791335e-04,  3.2020526e-03,\n",
       "        4.7626775e-03,  2.6369726e-03, -3.8603717e-03,  1.2881706e-03,\n",
       "        2.9903608e-03, -4.8310203e-03, -1.4569885e-03, -3.0591968e-03,\n",
       "        1.0488429e-03,  1.0248466e-03, -2.4605021e-04, -2.4717415e-03,\n",
       "       -4.7942926e-03,  4.9163643e-03, -4.7331420e-03, -4.6979589e-03,\n",
       "        2.7942513e-03, -1.8443220e-03,  2.7057240e-03, -1.4493654e-03,\n",
       "        3.7698748e-03,  1.7548925e-03, -3.3299976e-03,  1.8733037e-03,\n",
       "       -9.1928232e-04,  2.7322567e-03,  2.1748156e-03,  2.1747733e-03,\n",
       "        4.9566239e-04, -1.0085618e-03,  8.3983043e-04,  1.3844452e-03,\n",
       "        4.3324027e-03,  3.4872149e-03, -1.5146798e-03,  4.0071188e-03,\n",
       "       -3.2751269e-03,  2.2665281e-03, -3.0617339e-03,  1.6188447e-03,\n",
       "        1.8928888e-03, -4.8020077e-03, -4.4406848e-03, -2.7960213e-03,\n",
       "       -2.6115424e-03, -6.1089057e-04,  1.6947509e-03,  2.8572548e-03,\n",
       "        1.9557037e-05, -8.0671918e-04,  4.2294175e-03,  2.8137749e-03,\n",
       "       -2.9135571e-04, -4.4492274e-03, -2.2587127e-03, -4.3312781e-03,\n",
       "        4.3369150e-03,  2.6818637e-03, -1.1963781e-03, -2.7329687e-03,\n",
       "        3.3418532e-04,  1.1221741e-03, -2.2282174e-03,  2.4169125e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec is done!\n",
    "model[mytokenizedlist[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp C:\\Users\\adity\\stanfordnlp_resources\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-fb49bd97a193473b.props -preload openie\n"
     ]
    }
   ],
   "source": [
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE() as client:\n",
    "    text = 'Barack ate a chicken'\n",
    "    mydick=(client.annotate(text)[0])            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicken'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydick[\"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
