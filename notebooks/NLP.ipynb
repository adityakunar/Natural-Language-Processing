{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the data\n",
    "roughDat= pd.read_csv(\"../data/PlayDat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes unnecessary columns.\n",
    "roughDat = roughDat.drop(columns=['Unnamed: 0', 'articleHeadline', 'claimId',\n",
    "       'articleId', 'articleHeadlineStance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks in the data if there is a question mark in the end.\n",
    "QFeature=[]\n",
    "for i in roughDat['claimHeadline']:\n",
    "    if i[-1]==\"?\":\n",
    "        QFeature.append(1)\n",
    "    else:\n",
    "        QFeature.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appends the QFeature to the claimHeadline dataframe.\n",
    "roughDat[\"QFeature\"]= QFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimHeadline</th>\n",
       "      <th>QFeature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Developers have limited access to Apple Watch ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Developers have limited access to Apple Watch ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Apple Watch only receives notifications wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple have different Watch bands for sale at l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Washington Post is developing a new app th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         claimHeadline  QFeature\n",
       "0    Developers have limited access to Apple Watch ...         0\n",
       "1    Developers have limited access to Apple Watch ...         0\n",
       "2    The Apple Watch only receives notifications wh...         0\n",
       "3    Apple have different Watch bands for sale at l...         0\n",
       "4    The Washington Post is developing a new app th...         0\n",
       "..                                                 ...       ...\n",
       "172  A woman lost her eye after being hit with a be...         0\n",
       "173  A woman lost her eye after being hit with a be...         0\n",
       "174  A woman lost her eye after being hit with a be...         0\n",
       "175  A woman lost her eye after being hit with a be...         0\n",
       "176  A woman lost her eye after being hit with a be...         0\n",
       "\n",
       "[177 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first feature has been extracted.\n",
    "roughDat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights into bag of words\n",
    "The BOW model only considers if a known word occurs in a document or not. It does not care about meaning, context, and order in which they appear.\n",
    "This gives the insight that similar documents will have word counts similar to each other. In other words, the more similar the words in two documents, the more similar the documents can be.\n",
    "Limitations of BOW\n",
    "Semantic meaning: the basic BOW approach does not consider the meaning of the word in the document. It completely ignores the context in which it’s used. The same word can be used in multiple places based on the context or nearby words.\n",
    "Vector size: For a large document, the vector size can be huge resulting in a lot of computation and time. You may need to ignore words based on relevance to your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "allsentences=list(roughDat['claimHeadline'])\n",
    "X = vectorizer.fit_transform(allsentences)\n",
    "\n",
    "#if we want to convert our occurences to frequencies\n",
    "#tf_transformer = TfidfTransformer(use_idf=False).fit(X)\n",
    "#X_f = tf_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BogF=X_f.toarray()\n",
    "BogF=X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughDat[\"BoGF\"]= list(BogF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimHeadline</th>\n",
       "      <th>QFeature</th>\n",
       "      <th>BoGF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Developers have limited access to Apple Watch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Developers have limited access to Apple Watch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Apple Watch only receives notifications wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple have different Watch bands for sale at l...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Washington Post is developing a new app th...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>A woman lost her eye after being hit with a be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         claimHeadline  QFeature  \\\n",
       "0    Developers have limited access to Apple Watch ...         0   \n",
       "1    Developers have limited access to Apple Watch ...         0   \n",
       "2    The Apple Watch only receives notifications wh...         0   \n",
       "3    Apple have different Watch bands for sale at l...         0   \n",
       "4    The Washington Post is developing a new app th...         0   \n",
       "..                                                 ...       ...   \n",
       "172  A woman lost her eye after being hit with a be...         0   \n",
       "173  A woman lost her eye after being hit with a be...         0   \n",
       "174  A woman lost her eye after being hit with a be...         0   \n",
       "175  A woman lost her eye after being hit with a be...         0   \n",
       "176  A woman lost her eye after being hit with a be...         0   \n",
       "\n",
       "                                                  BoGF  \n",
       "0    [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1    [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "172  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "173  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "174  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "175  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "176  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[177 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roughDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CORENLP_HOME\"] = r'D:/Downloads/stanford-corenlp-full-2018-10-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "input text\n",
      "\n",
      "Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\n",
      "---\n",
      "starting up Java Stanford CoreNLP Server...\n",
      "Starting server with command: java -Xmx16G -cp D:/Downloads/stanford-english-corenlp-2018-10-05-models/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-db4e01b70b9e4c80.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n"
     ]
    }
   ],
   "source": [
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "# example text\n",
    "print('---')\n",
    "print('input text')\n",
    "print('')\n",
    "\n",
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "# set up the client\n",
    "print('---')\n",
    "print('starting up Java Stanford CoreNLP Server...')\n",
    "\n",
    "# set up the client\n",
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G') as client:\n",
    "    # submit the request to the server\n",
    "    ann = client.annotate(text)\n",
    "\n",
    "    # get the first sentence\n",
    "    sentence = ann.sentence[0]\n",
    "    \n",
    "    # get the constituency parse of the first sentence\n",
    "    print('---')\n",
    "    print('constituency parse of first sentence')\n",
    "    constituency_parse = sentence.parseTree\n",
    "    print(constituency_parse)\n",
    "\n",
    "    # get the first subtree of the constituency parse\n",
    "    print('---')\n",
    "    print('first subtree of constituency parse')\n",
    "    print(constituency_parse.child[0])\n",
    "\n",
    "    # get the value of the first subtree\n",
    "    print('---')\n",
    "    print('value of first subtree of constituency parse')\n",
    "    print(constituency_parse.child[0].value)\n",
    "\n",
    "    # get the dependency parse of the first sentence\n",
    "    print('---')\n",
    "    print('dependency parse of first sentence')\n",
    "    dependency_parse = sentence.basicDependencies\n",
    "    print(dependency_parse)\n",
    "\n",
    "    # get the first token of the first sentence\n",
    "    print('---')\n",
    "    print('first token of first sentence')\n",
    "    token = sentence.token[0]\n",
    "    print(token)\n",
    "\n",
    "    # get the part-of-speech tag\n",
    "    print('---')\n",
    "    print('part of speech tag of token')\n",
    "    token.pos\n",
    "    print(token.pos)\n",
    "\n",
    "    # get the named entity tag\n",
    "    print('---')\n",
    "    print('named entity tag of token')\n",
    "    print(token.ner)\n",
    "\n",
    "    # get an entity mention from the first sentence\n",
    "    print('---')\n",
    "    print('first entity mention in sentence')\n",
    "    print(sentence.mentions[0])\n",
    "\n",
    "    # access the coref chain\n",
    "    print('---')\n",
    "    print('coref chains for the example')\n",
    "    print(ann.corefChain)\n",
    "\n",
    "    # Use tokensregex patterns to find who wrote a sentence.\n",
    "    pattern = '([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/'\n",
    "    matches = client.tokensregex(text, pattern)\n",
    "    # sentences contains a list with matches for each sentence.\n",
    "    assert len(matches[\"sentences\"]) == 3\n",
    "    # length tells you whether or not there are any matches in this\n",
    "    assert matches[\"sentences\"][1][\"length\"] == 1\n",
    "    # You can access matches like most regex groups.\n",
    "    matches[\"sentences\"][1][\"0\"][\"text\"] == \"Chris wrote a simple sentence\"\n",
    "    matches[\"sentences\"][1][\"0\"][\"1\"][\"text\"] == \"Chris\"\n",
    "\n",
    "    # Use semgrex patterns to directly find who wrote what.\n",
    "    pattern = '{word:wrote} >nsubj {}=subject >dobj {}=object'\n",
    "    matches = client.semgrex(text, pattern)\n",
    "    # sentences contains a list with matches for each sentence.\n",
    "    assert len(matches[\"sentences\"]) == 3\n",
    "    # length tells you whether or not there are any matches in this\n",
    "    assert matches[\"sentences\"][1][\"length\"] == 1\n",
    "    # You can access matches like most regex groups.\n",
    "    matches[\"sentences\"][1][\"0\"][\"text\"] == \"wrote\"\n",
    "    matches[\"sentences\"][1][\"0\"][\"$subject\"][\"text\"] == \"Chris\"\n",
    "    matches[\"sentences\"][1][\"0\"][\"$object\"][\"text\"] == \"sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\adity\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\adity\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\adity\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\adity\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\adity\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\adity\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "('Barack', '5', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '5', 'aux:pass')\n",
      "('not', '5', 'advmod')\n",
      "('born', '0', 'root')\n",
      "('in', '7', 'case')\n",
      "('Hawaii', '5', 'obl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\cuda\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "doc = nlp(\"Barack Obama was not born in Hawaii\")\n",
    "a  = doc.sentences[0]\n",
    "a.print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
