{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "train = pd.read_csv('../../data/raw/Emergent_NAACL2016/emergent/url-versions-2015-06-14-clean-train.csv')\n",
    "train.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features\n",
    "align_feature = pd.read_csv('../../data/processed/features/alignment_feature.csv')\n",
    "cosine_feature = pd.read_csv('../../data/processed/features/cosine_feature.csv')\n",
    "bow_feature = pd.read_csv('../../data/processed/features/headline_BoWBigram.csv')\n",
    "qmark_feature = pd.read_csv('../../data/processed/features/headline_Qmark.csv')\n",
    "neg_alignment_feature = pd.read_csv('../../data/processed/features/neg_alignment_feature.csv')\n",
    "root_dist_feature = pd.read_csv('../../data/processed/features/root_dist_min.csv')\n",
    "svo_feature = pd.read_csv('../../data/processed/features/svo_Lexical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_map = {\"for\": 0, \"observing\": 1, \"against\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.replace({\"articleHeadlineStance\": target_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "for          992\n",
       "observing    775\n",
       "against      304\n",
       "Name: articleHeadlineStance, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"articleHeadlineStance\"].value_counts()#,test[\"articleHeadlineStance\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, align_feature, on=\"articleId\")\n",
    "train = pd.merge(train, cosine_feature, on=\"articleId\")\n",
    "train = pd.merge(train, bow_feature, on=\"articleId\")\n",
    "train = pd.merge(train, qmark_feature, on=\"articleId\")\n",
    "train = pd.merge(train, neg_alignment_feature, on=\"articleId\")\n",
    "train = pd.merge(train, root_dist_feature, on=\"articleId\")\n",
    "# train = pd.merge(train, svo_feature, on=\"articleId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"articleHeadlineStance\"] = train[\"articleHeadlineStance\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets and shuuffle with random seed\n",
    "seed = 1234\n",
    "\n",
    "train = sklearn.utils.shuffle(train, random_state=seed)\n",
    "#test = sklearn.utils.shuffle(test, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test\n",
    "\n",
    "X_train = train.to_numpy()[:,5:]\n",
    "Y_train = train[\"articleHeadlineStance\"].values.reshape((-1,))\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_train, Y_train, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "#X_test = test.to_numpy()[:,5:]\n",
    "#Y_test = test[\"articleHeadlineStance\"].values.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train = Y_Train\n",
    "# X_train = X_Train.astype(float)\n",
    "# X_val = X_Test.astype(float)\n",
    "# Y_val = Y_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision is how many times we idenity something as postive and it's actually positive.\n",
    "Recall is in the number of positive cases, how many did we get right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: (1863,) TEST: (208,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n",
      "TRAIN: (1864,) TEST: (207,)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, )\n",
    "skf.get_n_splits(X_train, Y_train)\n",
    "\n",
    "\n",
    "train_folds = []\n",
    "valid_folds = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_train, Y_train)):\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    x_train_fold, x_valid_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_valid_fold = Y_train[train_index], Y_train[test_index]\n",
    "    \n",
    "    '''bc = np.bincount(y_train_fold)\n",
    "    ii = np.nonzero(bc)[0]\n",
    "    print(\"train set distribution\")\n",
    "    print(list(zip(ii,bc[ii])))\n",
    "    bc = np.bincount(y_valid_fold)\n",
    "    ii = np.nonzero(bc)[0]\n",
    "    print(\"validation set distribution\")\n",
    "    print(list(zip(ii,bc[ii])))\n",
    "    print(\"----------------------------\")\n",
    "    '''\n",
    "    train_fold = np.concatenate((x_train_fold, y_train_fold.reshape(-1, 1)), axis=1)\n",
    "    valid_fold = np.concatenate((x_valid_fold, y_valid_fold.reshape(-1, 1)), axis=1)\n",
    "    train_folds.append(train_fold)\n",
    "    valid_folds.append(valid_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 208 points : 46\n",
      "Accuracy on test set: 0.7788461538461539\n",
      "F1 Score [0.69090909 0.83333333 0.73103448]\n",
      "Number of mislabeled points out of a total 207 points : 52\n",
      "Accuracy on test set: 0.748792270531401\n",
      "F1 Score [0.68       0.80717489 0.68085106]\n",
      "Number of mislabeled points out of a total 207 points : 46\n",
      "Accuracy on test set: 0.7777777777777778\n",
      "F1 Score [0.79310345 0.83257919 0.68148148]\n",
      "Number of mislabeled points out of a total 207 points : 48\n",
      "Accuracy on test set: 0.7681159420289855\n",
      "F1 Score [0.83636364 0.79620853 0.7027027 ]\n",
      "Number of mislabeled points out of a total 207 points : 43\n",
      "Accuracy on test set: 0.7922705314009661\n",
      "F1 Score [0.84615385 0.8125     0.73913043]\n",
      "Number of mislabeled points out of a total 207 points : 45\n",
      "Accuracy on test set: 0.782608695652174\n",
      "F1 Score [0.73469388 0.81690141 0.75      ]\n",
      "Number of mislabeled points out of a total 207 points : 47\n",
      "Accuracy on test set: 0.7729468599033816\n",
      "F1 Score [0.79245283 0.80751174 0.71621622]\n",
      "Number of mislabeled points out of a total 207 points : 47\n",
      "Accuracy on test set: 0.7729468599033816\n",
      "F1 Score [0.77777778 0.81308411 0.71232877]\n",
      "Number of mislabeled points out of a total 207 points : 52\n",
      "Accuracy on test set: 0.748792270531401\n",
      "F1 Score [0.62962963 0.8115942  0.70588235]\n",
      "Number of mislabeled points out of a total 207 points : 49\n",
      "Accuracy on test set: 0.7632850241545893\n",
      "F1 Score [0.69230769 0.8        0.73469388]\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10e2\n",
    "C=0.8\n",
    "penalty='l1'\n",
    "\n",
    "# Normal Train and Test\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty=penalty, C=C, max_iter = max_iter, multi_class='auto', solver=\"liblinear\")\n",
    "running_acc=[]\n",
    "running_f1 = []\n",
    "for k,l in zip(train_folds,valid_folds):\n",
    "    train_X = k[:,:-1]\n",
    "    train_y = np.ravel(k[:,-1:])\n",
    "    valid_X = l[:,:-1]\n",
    "    valid_y = np.ravel(l[:,-1:])\n",
    "    ypred = model.fit(train_X, train_y).predict(valid_X)\n",
    "    f1 = f1_score(valid_y, ypred, average=None)\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\" % (valid_X.shape[0], (valid_y != ypred).sum()))\n",
    "    print(\"Accuracy on test set: \"+str(model.score(valid_X, valid_y)))\n",
    "    print(f\"F1 Score {f1}\")\n",
    "    running_acc.append(model.score(valid_X, valid_y))\n",
    "    running_f1.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc 0.770638238573021\n",
      "std acc 0.0132483005523329\n"
     ]
    }
   ],
   "source": [
    "print(\"mean acc \" + str(np.array(running_acc).mean()))\n",
    "print(\"std acc \" + str(np.array(running_acc).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean agf1 0.7473391829157234\n",
      "std agf1 0.06892650917804831\n",
      "mean forf1 0.813088739813975\n",
      "std forf1 0.011528967557117608\n",
      "mean obsf1 0.7154321379386902\n",
      "std obsf1 0.022322577140768067\n"
     ]
    }
   ],
   "source": [
    "against_f1 = [ i[0] for i in running_f1]\n",
    "for_f1 = [ i[1] for i in running_f1]\n",
    "observing_f1 = [ i[2] for i in running_f1]\n",
    "print(\"mean agf1 \" + str(np.array(against_f1).mean()))\n",
    "print(\"std agf1 \" + str(np.array(against_f1).std()))\n",
    "print(\"mean forf1 \" + str(np.array(for_f1).mean()))\n",
    "print(\"std forf1 \" + str(np.array(for_f1).std()))\n",
    "print(\"mean obsf1 \" + str(np.array(observing_f1).mean()))\n",
    "print(\"std obsf1 \" + str(np.array(observing_f1).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
