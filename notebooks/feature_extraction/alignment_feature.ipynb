{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from stanfordnlp.server import CoreNLPClient \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordnlp.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data\n",
    "df = pd.read_csv(\"../../data/raw/Emergent_NAACL2016/emergent/url-versions-2015-06-14-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into claims and headlines\n",
    "claims, headlines = np.split(df[[\"claimHeadline\", \"articleHeadline\"]].values, 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case and remove full stops\n",
    "claims = [re.sub(r\"([\\.\\?\\!]+$|[\\.\\?\\!]+\\ )\", \"\", claim.lower()) for claim in claims.flatten().tolist()]\n",
    "headlines = [re.sub(r\"([\\.\\?\\!]+$|[\\.\\?\\!]+\\ )\", \"\", headline.lower()) for headline in headlines.flatten().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_unique = [re.sub(r\"[\\.\\?\\!]+$\", \"\", claim.lower()) for claim in df[\"claimHeadline\"].unique().tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize claims and headlines\n",
    "claims_tok = [word_tokenize(claim) for claim in claims]\n",
    "headlines_tok = [word_tokenize(headline) for headline in headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem claims\n",
    "claims_tok_stemmed = []\n",
    "for claim in claims_tok:\n",
    "    claim_tok_stemmed = []\n",
    "    for tok in claim:\n",
    "        claim_tok_stemmed.append(stemmer.stem(tok))\n",
    "    claims_tok_stemmed.append(claim_tok_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem headlines\n",
    "headlines_tok_stemmed = []\n",
    "for headline in headlines_tok:\n",
    "    headline_tok_stemmed = []\n",
    "    for tok in headline:\n",
    "        headline_tok_stemmed.append(stemmer.stem(tok))\n",
    "    headlines_tok_stemmed.append(headline_tok_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pairs of claim and headline tokens\n",
    "pairs_tok = []\n",
    "for claim_tok, headline_tok in zip(claims_tok, headlines_tok):\n",
    "    pairs_tok.append(list(itertools.product(claim_tok, headline_tok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pairs of claim and headline stemmed tokens\n",
    "pairs_tok_stemmed = []\n",
    "for claim_tok_stemmed, headline_tok_stemmed in zip(claims_tok_stemmed, headlines_tok_stemmed):\n",
    "    pairs_tok_stemmed.append(list(itertools.product(claim_tok_stemmed, headline_tok_stemmed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ppdb data\n",
    "with open(\"../../data/processed/ppdb/ppdb-small-all.pkl\", \"rb\") as f:\n",
    "    ppdb_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in a pair of\n",
    "# headline and claim tokens(stemmed and non-stemmed)\n",
    "# and returns their ppdb score\n",
    "def ppdb_func(stem_pair, token_pair, ppdb_dict, max_score=10, min_score=-10):\n",
    "    paraphrase = ppdb_dict.get(token_pair[0], False)\n",
    "    if stem_pair[0] == stem_pair[1]:\n",
    "        return max_score\n",
    "    elif paraphrase and token_pair[1] in paraphrase:\n",
    "        ppdb_score = paraphrase[token_pair[1]][0]\n",
    "        return ppdb_score\n",
    "    else:\n",
    "        return min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(pairs_tok)\n",
    "alignment_feature = []\n",
    "pwC=[]\n",
    "pwH=[]\n",
    "iC=[]\n",
    "iH=[]\n",
    "# iterate over all samples\n",
    "for i in range(num_samples):\n",
    "    # get tokens for current sample\n",
    "    sample_tok = pairs_tok[i]\n",
    "    sample_stem = pairs_tok_stemmed[i]\n",
    "    num_pairs = len(sample_tok)\n",
    "\n",
    "    # matrix of ppdb scores of each claim-headline token pair\n",
    "    # each row represents a claim\n",
    "    # each column represents a headline\n",
    "    # score_matrix[n][m]: ppdb score for nth claim and mth headline for the ith sample\n",
    "    score_matrix = []\n",
    "\n",
    "    # iterate over all token pairs in sample\n",
    "    for j in range(num_pairs):\n",
    "        # get current pair\n",
    "        stem_pair = sample_stem[j]\n",
    "        token_pair = sample_tok[j]\n",
    "\n",
    "        # when one claim token is done,\n",
    "        # move to the next row of the score matrix\n",
    "        if j%len(headlines_tok[i]) == 0:\n",
    "            score_matrix.append([])\n",
    "        # get ppdb score between the pair of claim and headline\n",
    "        score_matrix[-1].append(ppdb_func(stem_pair, token_pair, ppdb_dict))\n",
    "    # after scores between all pairs,\n",
    "    # convert score matrix to a numpy array\n",
    "    score_matrix = np.array(score_matrix)\n",
    "    # compute the optimal assignment of pairs,\n",
    "    # using the hungarian algorithm\n",
    "    # here, use the negative of the score matrix because we want to maximize scores\n",
    "    # (hungarian algorithm, by default tries to minimize the cost matrix)\n",
    "    row_ind, col_ind = linear_sum_assignment(-score_matrix)\n",
    "    \n",
    "    # norm: min(length of claim, length of headline)\n",
    "    norm = min(len(claims_tok[i]), len(headlines_tok[i]))\n",
    "    # use the indices returned from the hungarian algorithm,\n",
    "    # to get optimal assignment and sum to get score of max 1-1 alignment\n",
    "    alignment_feature.append(score_matrix[row_ind, col_ind].sum()/norm)\n",
    "    \n",
    "    # TODO: neg feature from the stanfordnlp dependency parse\n",
    "    pwC.append(np.array(claims_tok[i])[row_ind])\n",
    "    pwH.append(np.array(headlines_tok[i])[col_ind])\n",
    "    iC.append(row_ind)\n",
    "    iH.append(col_ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CORENLP_HOME\"] = r'D:\\\\Learning Material\\\\IR\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/nikilsaldanaha/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-b89c3de0f9f245db.props -preload tokenize,ssplit,pos,depparse\n"
     ]
    }
   ],
   "source": [
    "edgelistC=[]\n",
    "\n",
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:\n",
    "    for claim in claims:\n",
    "        claimc = client.annotate(claim)\n",
    "        edges= claimc.sentence[0].basicDependencies.edge\n",
    "        edgelistC.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/nikilsaldanaha/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-8f47016e10da4c1d.props -preload tokenize,ssplit,pos,depparse\n"
     ]
    }
   ],
   "source": [
    "edgelistH=[]\n",
    "\n",
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:\n",
    "    for headline in headlines:\n",
    "        headlinesh = client.annotate(headline)\n",
    "        edges= headlinesh.sentence[0].basicDependencies.edge\n",
    "        edgelistH.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "neg_features = []\n",
    "for claim_edges, headline_edges in zip(edgelistC, edgelistH):\n",
    "    neg_feature = 0\n",
    "    for c, h in zip(iC[i], iH[i]):\n",
    "        for e in claim_edges:\n",
    "            if (e.source==c)and(e.dep==\"neg\"):\n",
    "                neg_feature = 1\n",
    "                break\n",
    "                \n",
    "        for f in headline_edges:\n",
    "            if (f.source==h)and(f.dep==\"neg\"):\n",
    "                if neg_feature == 0:\n",
    "                    neg_feature = 1\n",
    "                else:\n",
    "                    neg_feature = 0\n",
    "                break\n",
    "        if neg_feature == 1:\n",
    "            break\n",
    "    neg_features.append(neg_feature)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alignment_feature_df = pd.DataFrame({\"articleId\": df[\"articleId\"].tolist(), \"negAlignmentScore\": neg_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_feature_df = pd.DataFrame({\"articleId\": df[\"articleId\"].tolist(), \"alignmentScore\": alignment_feature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_feature_df.to_csv(\"../../data/processed/features/alignment_feature.csv\", index=False)\n",
    "neg_alignment_feature_df.to_csv(\"../../data/processed/features/neg_alignment_feature.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
