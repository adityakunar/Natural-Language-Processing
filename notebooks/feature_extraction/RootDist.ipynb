{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the data\n",
    "# roughDat= pd.read_csv(\"PlayDat.csv\")\n",
    "roughDat= pd.read_csv(\"../../data/raw/Emergent_NAACL2016/emergent/url-versions-2015-06-14-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes unnecessary columns.\n",
    "# roughDat = roughDat.drop(columns=['Unnamed: 0', 'articleHeadline', 'claimId',\n",
    "#        'articleId', 'articleHeadlineStance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = r'D:\\\\Learning Material\\\\IR\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re \n",
    "mylist=roughDat[\"articleHeadline\"].tolist()\n",
    "#myshortlist = [re.sub(r'[^\\w\\s\\']','',s) for s in mylist] for removing punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp D:\\\\Learning Material\\\\IR\\\\stanford-corenlp-full-2018-10-05\\\\stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-87bead3037ef477e.props -preload depparse\n"
     ]
    }
   ],
   "source": [
    "from stanfordnlp.server import CoreNLPClient\n",
    "rootIndex =[]\n",
    "rootDistances = []\n",
    "with CoreNLPClient(annotators=['depparse'], timeout=60000, memory='16G') as client:\n",
    "    # submit the request to the server\n",
    "    for sentences in mylist:\n",
    "\n",
    "        ann = client.annotate(sentences)\n",
    "\n",
    "        # get the first sentence\n",
    "        sentence = ann.sentence[0]\n",
    "        # get the dependency parse of the first sentence\n",
    "        dependency_parse = sentence.basicDependencies\n",
    "        root=int(dependency_parse.root[0])\n",
    "        rootIndex.append(root)\n",
    "        neg_Pos=None\n",
    "        for i in dependency_parse.edge:\n",
    "            if i.dep==\"neg\":\n",
    "                neg_Pos=int(i.target)\n",
    "        if neg_Pos!=None:\n",
    "            rootDist=abs(root - neg_Pos)\n",
    "        else:\n",
    "            rootDist=0\n",
    "        rootDistances.append(rootDist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "refuting =['fake',\n",
    "           'fraud',\n",
    "           'hoax',\n",
    "           'false',\n",
    "           'deny', \n",
    "           'denies',\n",
    "           'refute',\n",
    "           'not',\n",
    "           'despite',\n",
    "           'nope',\n",
    "           'doubt', \n",
    "           'doubts',\n",
    "           'bogus',\n",
    "           'debunk',\n",
    "           'pranks',\n",
    "           'prank',\n",
    "           'retract']\n",
    "hedging = [\n",
    "        'alleged', 'allegedly',\n",
    "        'apparently',\n",
    "        'appear', 'appears',\n",
    "        'claim', 'claims',\n",
    "        'could',\n",
    "        'evidently',\n",
    "        'largely',\n",
    "        'likely',\n",
    "        'mainly',\n",
    "        'may', 'maybe', 'might',\n",
    "        'mostly',\n",
    "        'perhaps',\n",
    "        'presumably',\n",
    "        'probably',\n",
    "        'purported', 'purportedly',\n",
    "        'reported', 'reportedly',\n",
    "        'rumor', 'rumour', 'rumors', 'rumours', 'rumored', 'rumoured',\n",
    "        'says',\n",
    "        'seem',\n",
    "        'somewhat',\n",
    "        'supposedly',\n",
    "        'unconfirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytokenizedlist =[word_tokenize(i) for i in mylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedged_RDist = []\n",
    "k = 0\n",
    "for i in mytokenizedlist:\n",
    "    Distances = []\n",
    "    for j in hedging:\n",
    "        if j in i:\n",
    "            Dist = abs(rootIndex[k]-i.index(j))\n",
    "            Distances.append(Dist)\n",
    "        else:\n",
    "            Distances.append(0)\n",
    "    k+=1\n",
    "    Distances = [i for i in Distances if i != 0]\n",
    "    if len(Distances)==0:\n",
    "        Distances.append(0)\n",
    "    hedged_RDist.append(min(Distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "refute_RDist =[]\n",
    "k=0\n",
    "for i in mytokenizedlist:\n",
    "    Distances=[]\n",
    "    for j in refuting:\n",
    "        if j in i:\n",
    "            Dist=(abs(rootIndex[k]-i.index(j)))\n",
    "            Distances.append(Dist)\n",
    "        else:\n",
    "            Distances.append(0)\n",
    "    k+=1\n",
    "    Distances = [i for i in Distances if i != 0]\n",
    "    if len(Distances)==0:\n",
    "        Distances.append(0)\n",
    "    refute_RDist.append(min(Distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedged_RDist = np.array(hedged_RDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "refute_RDist = np.array(refute_RDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dist_feature_df = pd.DataFrame({\"articleId\": roughDat[\"articleId\"].tolist(), \"rootDistance\": rootDistances, \"hedgedRootDistance\": hedged_RDist, \"refuteRootDistance\": refute_RDist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dist_feature_df.to_csv(\"../../data/processed/features/root_dist.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
